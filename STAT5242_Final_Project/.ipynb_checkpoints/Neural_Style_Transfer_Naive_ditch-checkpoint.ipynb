{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2473,
     "status": "ok",
     "timestamp": 1608580670820,
     "user": {
      "displayName": "Jiaxiang Wang",
      "photoUrl": "",
      "userId": "02968681471987406290"
     },
     "user_tz": 360
    },
    "id": "LdJt0WVQoxPO",
    "outputId": "a41efdb9-3aa5-405c-f08d-3eca22318197"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.gridspec as gridspec\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h_MD_IWcoxID"
   },
   "outputs": [],
   "source": [
    "def get_noise_image(content_img, seed):\n",
    "    np.random.seed(seed)\n",
    "    img = np.random.uniform(0, 1, content_img.shape).astype(np.float32)\n",
    "    return img\n",
    "\n",
    "\n",
    "def load_and_process_image(content_path, style_path):\n",
    "    content_img = Image.open(content_path)\n",
    "    content_img = np.array(content_img)\n",
    "    content_shape = content_img.shape[:2]\n",
    "    content_shape = content_shape[::-1]\n",
    "    \n",
    "    style_img = Image.open(style_path)\n",
    "    style_img = style_img.resize(content_shape)\n",
    "    style_img = np.array(style_img)\n",
    "\n",
    "    content_img = tf.expand_dims(content_img, axis=0)\n",
    "    style_img = tf.expand_dims(style_img, axis=0)\n",
    "    \n",
    "    return content_img, style_img\n",
    "\n",
    "# ???\n",
    "def deprocess_image(processed_img):\n",
    "    x = processed_img.copy()\n",
    "    if len(x.shape) == 4:\n",
    "        x = np.squeeze(x, 0)\n",
    "  \n",
    "    # perform the inverse of the preprocessing step\n",
    "    x[:, :, 0] += 103.939\n",
    "    x[:, :, 1] += 116.779\n",
    "    x[:, :, 2] += 123.68\n",
    "    x = x[:, :, ::-1]\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n",
    "def display_result(content_path, style_path, best_img=None):\n",
    "    content_img, style_img = load_and_process_image(content_path, style_path)\n",
    "\n",
    "    if len(content_img.shape) > 3:\n",
    "      content_img = np.squeeze(content_img, axis=0)\n",
    "\n",
    "    if len(style_img.shape) > 3:\n",
    "      style_img = np.squeeze(style_img, axis=0)\n",
    "    \n",
    "    fig = plt.figure(figsize=(13,13))\n",
    "    gridspec.GridSpec(2,2)\n",
    "\n",
    "    plt.subplot2grid((3,2), (0,0))\n",
    "    plt.imshow(content_img)\n",
    "    plt.title('Content Image')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot2grid((3,2), (0,1))\n",
    "    plt.imshow(style_img)\n",
    "    plt.title('Style Image')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    if best_img is not None:\n",
    "        plt.subplot2grid((3,2), (1,0), colspan=2, rowspan=2)\n",
    "        plt.imshow(best_img)\n",
    "        plt.title('Style Transfer Output')\n",
    "        plt.axis('off')\n",
    "\n",
    "    fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q5yaGF8woxE8"
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    \n",
    "    # VGG19 based \n",
    "    vgg19 = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n",
    "    vgg19.trainable = False\n",
    "    \n",
    "    content_layers = ['block5_conv2']\n",
    "    style_layers = ['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1', 'block5_conv1']\n",
    "\n",
    "    content_outputs = [vgg19.get_layer(name).output for name in content_layers]\n",
    "    style_outputs = [vgg19.get_layer(name).output for name in style_layers]\n",
    "\n",
    "    model_outputs = content_outputs + style_outputs\n",
    "\n",
    "    model = Model(inputs=vgg19.input, outputs=model_outputs)\n",
    "    \n",
    "    return model\n",
    "    \n",
    "\n",
    "def feature_extract(model, img, mode, num_content_layers=1):\n",
    "    outputs = model(img)\n",
    "\n",
    "    if mode == 'content':\n",
    "      feature = [out for out in outputs[:num_content_layers]]\n",
    "    elif mode == 'style':\n",
    "      feature = [out for out in outputs[num_content_layers:]]\n",
    "    \n",
    "    return feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qXZBU_H4oxBh"
   },
   "outputs": [],
   "source": [
    "def get_content_loss(base_content, target_content):\n",
    "    return tf.reduce_mean(tf.square(base_content - target_content))\n",
    "\n",
    "\n",
    "def get_style_loss(base_style, target_style):\n",
    "    return tf.reduce_mean(tf.square(base_style - target_style))\n",
    "    \n",
    "\n",
    "def compute_loss(model, base_img, target_content, target_style, content_weight, style_weight):\n",
    "    \n",
    "    content_loss = 0\n",
    "    style_loss = 0\n",
    "    \n",
    "    base_content = feature_extract(model, base_img, mode='content')\n",
    "    base_style = feature_extract(model, base_img, mode='style')\n",
    "    \n",
    "    layer_weight_content = 1/len(target_content)\n",
    "    for bc, tc in zip(base_content, target_content):\n",
    "        content_loss += layer_weight_content * get_content_loss(bc, tc)\n",
    "        \n",
    "    layer_weight_style = 1/len(target_style)\n",
    "    for bs, ts in zip(base_style, target_style):\n",
    "        style_loss += layer_weight_style * get_style_loss(bs, ts)\n",
    "        \n",
    "    content_loss *= content_weight\n",
    "    style_loss *= style_weight\n",
    "    \n",
    "    total_loss = content_loss + style_loss\n",
    "    \n",
    "    return total_loss, content_loss, style_loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YOsoJNqQow-i"
   },
   "outputs": [],
   "source": [
    "def compute_grad(config):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(config['base_img'])\n",
    "        total_loss = compute_loss(**config)[0]\n",
    "    return tape.gradient(total_loss, config['base_img']), total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KYScrW6how7V"
   },
   "outputs": [],
   "source": [
    "def style_transfer(content_path, style_path, content_weight, style_weight, num_iterations=500):\n",
    "    model = build_model()\n",
    "\n",
    "    content_img, style_img = load_and_process_image(content_path, style_path)\n",
    "    \n",
    "    content_img = tf.keras.applications.vgg19.preprocess_input(content_img)\n",
    "    style_img = tf.keras.applications.vgg19.preprocess_input(style_img)\n",
    "\n",
    "    base_img = tf.identity(content_img)\n",
    "    base_img = tf.cast(base_img, dtype='float')\n",
    "    base_img = tf.Variable(base_img)\n",
    "\n",
    "    noise_img = get_noise_image(content_img, 0)\n",
    "    base_img = tf.Variable(noise_img.copy())\n",
    "    \n",
    "    target_content = feature_extract(model, content_img, mode='content')\n",
    "    target_style = feature_extract(model, style_img, mode='style')\n",
    "    \n",
    "    config = {'model': model,\n",
    "              'base_img': base_img,\n",
    "              'target_content': target_content,\n",
    "              'target_style': target_style,\n",
    "              'content_weight': content_weight,\n",
    "              'style_weight': style_weight}\n",
    "    \n",
    "    norm_means = np.array([103.939, 116.779, 123.68])\n",
    "    min_vals = -norm_means\n",
    "    max_vals = 255 - norm_means \n",
    "    \n",
    "    best_loss, best_image = float('inf'), None\n",
    "    \n",
    "    optim = tf.keras.optimizers.Adam(learning_rate=5, beta_1=0.99, epsilon=0.1)\n",
    "\n",
    "    total_loss_hist = np.zeros(num_iterations)\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        grad, total_loss = compute_grad(config)\n",
    "        total_loss_hist[i] = total_loss\n",
    "        optim.apply_gradients([(grad, base_img)])\n",
    "        # ???\n",
    "        clipped_img = tf.clip_by_value(base_img, min_vals, max_vals)\n",
    "        base_img.assign(clipped_img)\n",
    "        \n",
    "        if total_loss < best_loss:\n",
    "            best_loss = total_loss\n",
    "            best_img = deprocess_image(base_img.numpy())\n",
    "        \n",
    "    return best_loss, best_img, total_loss_hist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kP7piWdRow1b"
   },
   "outputs": [],
   "source": [
    "content_path = 'Image/content1.jpg'\n",
    "style_path = 'Image/style1.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "P3octZWOowyK"
   },
   "outputs": [],
   "source": [
    "content_weight = 1e3\n",
    "style_weight = 1e-2\n",
    "num_iterations = 1000\n",
    "\n",
    "best_loss, best_img, total_loss_hist = style_transfer(content_path, style_path, content_weight, style_weight, num_iterations=num_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RDzYQhOsW7sn"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "ax.plot(range(1,num_iterations), total_loss_hist[1:], alpha=0.6, label='Total Loss')\n",
    "plt.xlabel('Number of Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss vs Number of Iterations')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 945,
     "output_embedded_package_id": "1uRhl7L_3J7skZYx0heJ_xrFYouioIiln"
    },
    "executionInfo": {
     "elapsed": 3538,
     "status": "ok",
     "timestamp": 1608580536092,
     "user": {
      "displayName": "Jiaxiang Wang",
      "photoUrl": "",
      "userId": "02968681471987406290"
     },
     "user_tz": 360
    },
    "id": "G33pshusomCr",
    "outputId": "80f5f667-1360-4663-cfc3-6484595d2bf6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Output hidden; open in https://colab.research.google.com to view."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_result(content_path, style_path, best_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wuQ35A3qIb4f"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Neural_Style_Transfer_Naive_ditch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
